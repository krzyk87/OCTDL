#!/usr/bin/env python3
import os
import sys
import re
import csv
import argparse
from typing import List, Dict, Optional, Tuple

# Regex patterns to capture metrics in test output files
METRIC_PATTERNS = {
    "acc": re.compile(r"^acc:\s*([0-9]*\.?[0-9]+)", re.IGNORECASE),
    "f1": re.compile(r"^f1:\s*([0-9]*\.?[0-9]+)", re.IGNORECASE),
    "auc": re.compile(r"^auc:\s*([0-9]*\.?[0-9]+)", re.IGNORECASE),
    "precision": re.compile(r"^precision:\s*([0-9]*\.?[0-9]+)", re.IGNORECASE),
    "recall": re.compile(r"^recall:\s*([0-9]*\.?[0-9]+)", re.IGNORECASE),
}
CATEGORIES_PATTERN = re.compile(r"^Categories:\s*(\d+)")


def import_default_networks(project_root: str) -> List[str]:
    """Import NETWORKS from train_all_networks.py located at project root.

    Falls back to an empty list if import fails.
    """
    try:
        sys.path.insert(0, project_root)
        from train_all_networks import NETWORKS  # type: ignore
        return list(NETWORKS)
    except Exception as e:
        print(f"[aggregate] Warning: Could not import NETWORKS from train_all_networks.py: {e}")
        return []
    finally:
        if sys.path and sys.path[0] == project_root:
            sys.path.pop(0)


def parse_test_output(file_path: str) -> Tuple[Optional[int], Dict[str, Optional[float]]]:
    """Parse a test_output_*.txt file and extract categories count and metrics.

    Returns: (categories_count, metrics_dict)
    metrics_dict keys: acc, f1, auc, precision, recall
    Missing values will be None.
    """
    categories = None
    metrics: Dict[str, Optional[float]] = {k: None for k in METRIC_PATTERNS.keys()}

    if not os.path.exists(file_path):
        return categories, metrics

    with open(file_path, "r", encoding="utf-8", errors="ignore") as f:
        for raw_line in f:
            line = raw_line.strip()
            if not line:
                continue
            if categories is None:
                m = CATEGORIES_PATTERN.match(line)
                if m:
                    try:
                        categories = int(m.group(1))
                    except ValueError:
                        categories = None
                    continue
            for key, pattern in METRIC_PATTERNS.items():
                if metrics[key] is None:
                    m = pattern.match(line)
                    if m:
                        try:
                            metrics[key] = float(m.group(1))
                        except ValueError:
                            metrics[key] = None
                        break
    return categories, metrics


def build_rows(dataset: str, networks: List[str], runs_dir: str, weights_suffix: str) -> List[Dict[str, Optional[str]]]:
    """Build rows for CSV/LaTeX tables by scanning each network's test_output file."""
    rows = []
    for net in networks:
        run_dir = os.path.join(runs_dir, dataset, f"run_A_{net}")
        test_out = os.path.join(run_dir, f"test_output_{net}_{weights_suffix}.txt")
        categories, metrics = parse_test_output(test_out)
        if not any(v is not None for v in metrics.values()):
            print(f"[aggregate] Warning: No metrics found for {net} (missing file or unparsable): {test_out}")
        row = {
            "Dataset": dataset,
            "Model": net,
            "Labels count": str(categories) if categories is not None else "",
            "Accuracy": f"{metrics['acc']:.6f}" if metrics["acc"] is not None else "",
            "Precision": f"{metrics['precision']:.6f}" if metrics["precision"] is not None else "",
            "Recall": f"{metrics['recall']:.6f}" if metrics["recall"] is not None else "",
            "F1-score": f"{metrics['f1']:.6f}" if metrics["f1"] is not None else "",
        }
        rows.append(row)
    return rows


def save_csv(rows: List[Dict[str, Optional[str]]], out_path: str) -> None:
    os.makedirs(os.path.dirname(out_path), exist_ok=True)
    fieldnames = ["Dataset", "Model", "Accuracy", "Precision", "Recall", "F1-score"]
    with open(out_path, "w", newline="", encoding="utf-8") as f:
        writer = csv.DictWriter(f, fieldnames=fieldnames)
        writer.writeheader()
        for r in rows:
            writer.writerow({k: r.get(k, "") for k in fieldnames})
    print(f"[aggregate] CSV saved to: {out_path}")


def save_latex(rows: List[Dict[str, Optional[str]]], out_path: str) -> None:
    os.makedirs(os.path.dirname(out_path), exist_ok=True)
    headers = ["Dataset", "Model", "Labels count", "Accuracy", "Precision", "Recall", "F1-score"]
    # Simple tabular environment; users can wrap it into table environment if desired
    col_spec = "l l c c c c c"
    lines = []
    lines.append("% Auto-generated by paper_docs/aggregate_test_results.py")
    lines.append("\\begin{tabular}{" + col_spec + "}")
    lines.append("\\hline")
    lines.append(" \\textbf{" + "} & \\textbf{".join(headers) + "} \\ ")
    lines.append("\\hline")
    for r in rows:
        vals = [r.get(h, "") or "" for h in headers]
        # Replace underscores in model names for LaTeX safety
        vals = [v.replace("_", "\\_") for v in vals]
        lines.append(" " + " & ".join(vals) + " \\ ")
    lines.append("\\hline")
    lines.append("\\end{tabular}")

    with open(out_path, "w", encoding="utf-8") as f:
        f.write("\n".join(lines) + "\n")
    print(f"[aggregate] LaTeX saved to: {out_path}")


def parse_args():
    p = argparse.ArgumentParser(description="Aggregate test results into CSV and LaTeX tables.")
    p.add_argument("--dataset-name", required=True, help="Dataset name (matches runs/{dataset-name})")
    p.add_argument(
        "--networks",
        nargs="*",
        help="Optional list of network names. If omitted, imported from train_all_networks.NETWORKS",
    )
    p.add_argument(
        "--weights-suffix",
        default="best",
        help="Suffix used in test output filenames: test_output_{net}_{suffix}.txt (default: best)",
    )
    return p.parse_args()


def main():
    args = parse_args()
    project_root = os.path.dirname(os.path.abspath(__file__))
    project_root = os.path.dirname(project_root)  # move from paper_docs/ to project root

    if args.networks and len(args.networks) > 0:
        networks = args.networks
    else:
        networks = import_default_networks(project_root)
        if not networks:
            print("[aggregate] Error: No networks provided and failed to import defaults.")
            sys.exit(1)

    runs_dir = os.path.join(project_root, "runs")
    dataset = args.dataset_name

    rows = build_rows(dataset, networks, runs_dir, args.weights_suffix)

    # Save outputs
    out_dir = os.path.join(runs_dir, dataset)
    csv_path = os.path.join(out_dir, f"results_{dataset}.csv")
    tex_path = os.path.join(out_dir, f"results_{dataset}.tex")

    save_csv(rows, csv_path)
    save_latex(rows, tex_path)

    # Brief summary for console
    print("[aggregate] Done. Rows aggregated:", len(rows))


if __name__ == "__main__":
    main()
